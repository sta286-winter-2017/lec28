\documentclass[ignorenonframetext,aspectratio=169]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\else % if luatex or xelatex
\ifxetex
\usepackage{mathspec}
\else
\usepackage{fontspec}
\fi
\defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight0.8\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
\let\insertpartnumber\relax
\let\partname\relax
\frame{\partpage}
}
\AtBeginSection{
\ifbibliography
\else
\let\insertsectionnumber\relax
\let\sectionname\relax
\frame{\sectionpage}
\fi
}
\AtBeginSubsection{
\let\insertsubsectionnumber\relax
\let\subsectionname\relax
\frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand\P[1]{P{\left(#1\right)}}
\newcommand\F[1]{F_{\tiny{#1}}}
\newcommand\f[1]{f_{\tiny{#1}}}
\newcommand\p[1]{p_{\tiny{#1}}}
\newcommand\M[1]{M_{\tiny{#1}}}
\newcommand\V[1]{\text{Var}\!\left(#1\right)}
\newcommand\E[1]{E\!\left(#1\right)}
\newcommand\N[1]{N_{\tiny{#1}}}
\newcommand\ol{\overline}

\title{STA286 Lecture 28}
\author{Neil Montgomery}
\date{Last edited: 2017-03-29 10:06}

\begin{document}
\frame{\titlepage}

\begin{frame}{comparing two proportions}

One variable in the dataset with 0's and 1's; another variable splitting
observations into two groups.

The two populations are Bernoulli(\(p_1\)) and Bernoulli(\(p_2\)). The
independent samples are \(X_{11},\ldots,X_{1n_1}\) and
\(X_{21},\ldots,X_{2n_2}\)

Patented process:

\begin{enumerate}[<+->]
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate \(\theta = p_1 - p_2\).
\item
  Estimator \(\hat\theta = \hat p_1 - \hat p_2 = \ol{X_1} - \ol{X_2}\)
\item
  \(\V{\hat p_1 - \hat p_2} = \frac{p_1(1-p_1)}{n_1} + \frac{p_1(1-p_1)}{n_2}\)
\item
  Whoops! Don't know \(p_1\) or \(p_2\). So use \(\hat p_1\) and
  \(\hat p_2\) instead. Bam. Done.
\end{enumerate}

\pause Formula for 95\% interval:

\[(\hat p_1 - \hat p_2) \pm 1.96\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1} + \frac{\hat p_1(1- \hat p_1)}{n_2}}\]

\end{frame}

\begin{frame}{two non-robust confidence intervals}

Every procedure I have explained so far is \textit{robust} as long as
the sample size is large enough (except for the prediction interval
formula.)

In principle we could apply the patented procedure to estimate
\(\sigma^2\) with \(S^2\), using a \(\chi^2\) distribution.

We could also apply the patented procedure to estimate the ratio
\(\sigma_1^2/\sigma_2^2\) with \(S_1^2/S_2^2\) using an \(F\)
distribution.

But the results are well known to be non-robust, even with large sample
sizes, so I cannot recommend them for use.

\pause A modern computationally-intensive technique called
\textit{bootstrapping} is a good choice in these and other difficult
situations.

\end{frame}

\section{chiselling your own estimators onto stone
tablets}\label{chiselling-your-own-estimators-onto-stone-tablets}

\begin{frame}{fun fact from mathematics}

Suppose a twice-differentiable function \(f(x)\) has a critical value at
\(x_0\), and \(g(x)\) is strictly increasing and twice-differentiable.

Then \(g(f(x))\) also has a critical value at \(x_0\), and the sign of
its second derivative at \(x_0\) is the same as the sign of the second
derivative of \(f\) at \(x_0\).

\pause We'll use this because we need the fact that \(f(x)\) and
\(\log(f(x))\) take on maximum values at the same point.

\end{frame}

\begin{frame}[fragile]{estimating a proportion, from first principles -
I}

Here's a simulated sequence of 0's and 1's from a Bernoulli\((p)\)
distribution. I know what \((p)\), but you don't.

\begin{verbatim}
##  [1] 1 1 0 0 0 1 1 0 0 0
\end{verbatim}

What value of \(p\) between \(0\) and \(1\) is the \textit{most likely}
to have produce this sequence of \(4\) 1's and \(6\) 0's?

\pause The probability of getting this sample exactly is:

\begin{align*}
& p\cdot p\cdot (1-p)\cdot (1-p)\cdot (1-p)\cdot p\cdot p\cdot (1-p)\cdot (1-p)\cdot (1-p) \\
&= p^4(1-p)^6
\end{align*}

Let's call this function \(L(p)\).

\pause We could maximize \(L(p)\), but it's easier to maximize
\(\ell(p) = \log{L(p)}\)

\end{frame}

\begin{frame}{estimating a proportion, from first principles - II}

\begin{align*}
0 = \frac{d}{dp}\ell(p) &= \frac{d}{dp}\left(4\log(p) + 6\log(1-p)\right) = \frac{4}{p} - \frac{6}{1-p}\\
\onslide<2->{\frac{4}{p} &= \frac{6}{1-p} \Longrightarrow p = \frac{4}{10}}
\end{align*}

\pause The second derivative is negative, so this is a maximum.

\pause It would have been no harder to work in general, with \(k\) 1's
out of a sample of size \(n\), and maximizing \(L(p) = p^k(1-p)^{n-k}\)

\pause The same calculus gives the maximum at \(k/n\).

\pause This is exactly the same as \(\hat p\) that was used as
``obvious'' from before.

\end{frame}

\begin{frame}{``likelihood function'' for Bernoulli}

The p.m.f. of a Bernoulli\((p)\) is \(f(x;p) = p^x(1-p)^{1-x}\) with
\(x \in\{0,1\}\).

\pause Given a sequence \(\{x_1,\ldots,x_n\}\) of 0's and 1's, yet
another way of constructing \(L(p)\) is as follows:

\begin{align*}
L(p) &= \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} \onslide<3->{=p^{\sum_{i=1}^n x_i}(1-p)^{n - \sum_{i=1}^n x_i}}\\
\onslide<4->{&= \prod_{i=1}^n f(x_i;p)}\\
\onslide<5->{\text{Also:} \quad \ell (p) &= \log L(p) = \sum_{i=1}^n \log f(x_i;p)}
\end{align*}

\end{frame}

\begin{frame}{likelihood function in general}

Given a sequence of observations \(\{x_1,\ldots,x_n\}\) (``the data'')
from a random variable \(X\) with pmf or pdf \(f(x;\theta)\), a
likelihood function \(L(\theta) = L(x_1,\ldots,x_n;\theta)\) for the
parameter \(\theta\) is defined as (for any positive \(g\)):
\[L(\theta) = \underbrace{g(\mathbf{x}) \prod_{i=1}^n f(x_i;\theta)}_{\text{real definition}} \onslide<2->{\propto \underbrace{\prod_{i=1}^n f(x_i;\theta)}_{\text{easy definition}}}\]

\pause If \(X\) is discrete and \(f\) is a pmf, then \(L(\theta)\) is
literally the probability of the data given \(\theta\).

\pause If \(X\) is continuous and \(f\) is a pdf, then \(L(\theta)\) is
not a probability, but it still provides a useful ``index'' for
\(\theta\) values.

\end{frame}

\begin{frame}{likelihood as ``index'' in continuous case}

Suppose \(X\sim\text{Exp}(\lambda)\) and the data are: \(1, 3, 8\). A
likelihood for \(\lambda\) is:
\[L(1,3,8;\lambda) = \lambda^3e^{-\lambda(1+3+8)}=\lambda^3e^{-12\lambda}\]

\pause Consider three possible candidate guesses for the true value of
\(\lambda\): 0.1, 0.25, and 0.5.

\end{frame}

\begin{frame}{a possibly useless and confusing picture}

\includegraphics{lec28_files/figure-beamer/unnamed-chunk-2-1.pdf}

\end{frame}

\begin{frame}{likelihood as ``index'' in continuous case}

Suppose \(X\sim\text{Exp}(\lambda)\) and the data are: \(1, 3, 8\). A
likelihood for \(\lambda\) is:
\[L(1,3,8;\lambda) = \lambda^3e^{-\lambda(1+3+8)}=\lambda^3e^{-12\lambda}\]

\pause Consider three possible candidate guesses for the true value of
\(\lambda\): 0.1, 0.25, and 0.5.

\begin{align*}
L(1) &= 1^3e^{-12} = 6.1442124\times 10^{-6}\\
\onslide<3->{L(0.25) &= (0.25)^3e^{-12\cdot 0.25} = 7.7792294\times 10^{-4}}\onslide<5->{\longleftarrow \text{ Highest "likelihood"}}\\
\onslide<4->{L(0.1) &= (0.1)^3e^{-12\cdot 0.1} = 3.0119421\times 10^{-4}}
\end{align*}

\end{frame}

\begin{frame}{maximum likelihood ``estimate''}

The value of \(\theta\) that maximizes \(L(\theta)\) is called the
\textit{maximum likelihood estimate}.

\pause In many cases it is more convenient to maximize \(\ell(\theta)\).

\pause For example, suppose \(x_1,x_2,\ldots,x_n\) are data observed
from a \(X\sim N(\mu, 1)\) population. A likelihood for \(\mu\) is:

\[L(\mu) = (2\pi)^{n/2}\exp\left(-\frac{1}{2}\sum_{i=1}^n (x_i - \mu)^2\right)\]

\pause 

\[\ell(\mu) = \log L(\mu) = C - \frac{1}{2}\sum_{i=1}^n (x_i - \mu)^2\]

\pause To maximize:
\[0 = \frac{d}{d\mu}\ell(\mu) = \sum_{i=1}^n (x_i - \mu) \onslide<6->{\Longrightarrow \mu = \frac{\sum_{i=1}^n x_i}{n} = \ol{x}}\]

\end{frame}

\begin{frame}{the maximum likelihood estimator}

A final technicality. When you replace the data \(x_1,x_2,\ldots,x_n\)
with its ``model'', the sample: \(X_1,X_2,\ldots,X_n\), inside the
maximum likelihood estimate, you end up with the
\textit{maximum likelihood estimator}, or MLE.

Traditionally the MLE notation is the parameter-with-a-hat.

\pause For example, the maximum likelihood estimator for \(\mu\) using a
sample from a \(N(\mu,1)\) population is: \[\hat\mu = \ol{X}\]

Everything so far extends to vector parameters. For example (textbook
example 9.21), the maximum likelihood estimates given data
\(x_1,\ldots,x_n\) from a \(N(\mu, \sigma)\) population, the MLE for
\(\theta=(\mu,\sigma^2)\) are:
\[\hat\mu = \ol{X} \qquad \widehat{\sigma^2} = \frac{\sum_{i=1}^n \left(X_i-\ol{X}\right)^2}{n}\]

\end{frame}

\begin{frame}{properties of the maximum likelihood estimator}

In most cases, the MLE \(\hat\theta\) has all the following (amazing!)
properties:

\begin{enumerate}[<+->]
\def\labelenumi{\arabic{enumi}.}
\item
  it is asymptotically unbiased.
\item
  it is consistent.
\item
  it is ``invariant'', which means
  \(\widehat{h(\theta)} = h(\hat\theta)\) when \(h\) is a 1-1 function.
\item
  it is asymptotically normal.
\item
  if \(c\hat\theta\) is unbiased for some constant \(c\), then
  \(c\hat\theta\) is the unbiased estimator with the smallest variance
  (our ``gold standard''.)
\end{enumerate}

\end{frame}

\end{document}
